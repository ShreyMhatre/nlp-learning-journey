{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJWmhAeyTw3mrjBW2h3OnS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyMhatre/nlp-learning-journey/blob/main/NLP_Generative_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative networks"
      ],
      "metadata": {
        "id": "Ia7Lff6bGyvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please restart the Colab runtime by going to \"Runtime\" -> \"Restart runtime\" in the menu. After the runtime restarts, run the following cell to install the necessary libraries."
      ],
      "metadata": {
        "id": "VZobH81LHIty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4wRWgpeGXMI"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow_datasets==4.9.2 tensorflow==2.18.0 protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()"
      ],
      "metadata": {
        "id": "FW4JjXtQHMJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building character vocabulary"
      ],
      "metadata": {
        "id": "nzANwr1YHQF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do character-level tokenization, we need to pass char_level=True parameter:"
      ],
      "metadata": {
        "id": "f_2gVRs5IaYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True,lower=False)\n",
        "tokenizer.fit_on_texts([x['title'].numpy().decode('utf-8') for x in ds_train])"
      ],
      "metadata": {
        "id": "7qwwDKIIHQ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to use one special token to denote end of sequence, which we will call <eos>. Let's add it manually to the vocabulary:"
      ],
      "metadata": {
        "id": "M_4E9Bo4IXss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eos_token = len(tokenizer.word_index)+1\n",
        "tokenizer.word_index['<eos>'] = eos_token\n",
        "\n",
        "vocab_size = eos_token + 1"
      ],
      "metadata": {
        "id": "4JV-JGnlHUXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['Hello, world!'])"
      ],
      "metadata": {
        "id": "-zjouDIpHVlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a generative RNN to generate titles"
      ],
      "metadata": {
        "id": "tzBXfN9fHXHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def title_batch(x):\n",
        "    x = [t.numpy().decode('utf-8') for t in x]\n",
        "    z = tokenizer.texts_to_sequences(x)\n",
        "    z = tf.keras.preprocessing.sequence.pad_sequences(z)\n",
        "    return tf.one_hot(z,vocab_size), tf.one_hot(tf.concat([z[:,1:],tf.constant(eos_token,shape=(len(z),1))],axis=1),vocab_size)"
      ],
      "metadata": {
        "id": "1SeauHZyHYgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title_batch_fn(x):\n",
        "    x = x['title']\n",
        "    a,b = tf.py_function(title_batch,inp=[x],Tout=(tf.float32,tf.float32))\n",
        "    return a,b"
      ],
      "metadata": {
        "id": "SOPcG2E6HdYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Masking(input_shape=(None,vocab_size)),\n",
        "    keras.layers.LSTM(128,return_sequences=True),\n",
        "    keras.layers.Dense(vocab_size,activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy')\n",
        "\n",
        "model.fit(ds_train.batch(8).map(title_batch_fn))"
      ],
      "metadata": {
        "id": "fIWSSyN6HfU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating output"
      ],
      "metadata": {
        "id": "QJVvuxgpjtff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have trained the model, we want to use it to generate some output. First of all, we need a way to decode text represented by a sequence of token numbers. To do this, we could use tokenizer.sequences_to_texts function; however, it does not work well with character-level tokenization. Therefore we will take a dictionary of tokens from the tokenizer (called word_index), build a reverse map, and write our own decoding function:"
      ],
      "metadata": {
        "id": "zI2LUtYwknMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_map = {val:key for key, val in tokenizer.word_index.items()}\n",
        "\n",
        "def decode(x):\n",
        "    return ''.join([reverse_map[t] for t in x])"
      ],
      "metadata": {
        "id": "pzZz6QpXjuoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,size=100,start='Today '):\n",
        "        inp = tokenizer.texts_to_sequences([start])[0]\n",
        "        chars = inp\n",
        "        for i in range(size):\n",
        "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
        "            nc = tf.argmax(out)\n",
        "            if nc==eos_token:\n",
        "                break\n",
        "            chars.append(nc.numpy())\n",
        "            inp = inp+[nc]\n",
        "        return decode(chars)\n",
        "\n",
        "generate(model)"
      ],
      "metadata": {
        "id": "ruqqySKcjz_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling output during training"
      ],
      "metadata": {
        "id": "xR2CDmh6j26e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we do not have any useful metrics such as accuracy, the only way we can see that our model is getting better is by sampling generated string during training. To do it, we will use callbacks"
      ],
      "metadata": {
        "id": "hoUHEKrAlFBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_callback = keras.callbacks.LambdaCallback(\n",
        "  on_epoch_end = lambda batch, logs: print(generate(model))\n",
        ")\n",
        "\n",
        "model.fit(ds_train.batch(8).map(title_batch_fn),callbacks=[sampling_callback],epochs=3)"
      ],
      "metadata": {
        "id": "qiAfXkGGj3xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft text generation and temperature"
      ],
      "metadata": {
        "id": "1_HF-gMjj8Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_soft(model,size=100,start='Today ',temperature=1.0):\n",
        "        inp = tokenizer.texts_to_sequences([start])[0]\n",
        "        chars = inp\n",
        "        for i in range(size):\n",
        "            out = model(tf.expand_dims(tf.one_hot(inp,vocab_size),0))[0][-1]\n",
        "            probs = tf.exp(tf.math.log(out)/temperature).numpy().astype(np.float64)\n",
        "            probs = probs/np.sum(probs)\n",
        "            nc = np.argmax(np.random.multinomial(1,probs,1))\n",
        "            if nc==eos_token:\n",
        "                break\n",
        "            chars.append(nc)\n",
        "            inp = inp+[nc]\n",
        "        return decode(chars)\n",
        "\n",
        "words = ['Today ','On Sunday ','Moscow, ','President ','Little red riding hood ']\n",
        "\n",
        "for i in [0.3,0.8,1.0,1.3,1.8]:\n",
        "    print(f\"\\n--- Temperature = {i}\")\n",
        "    for j in range(5):\n",
        "        print(generate_soft(model,size=300,start=words[j],temperature=i))"
      ],
      "metadata": {
        "id": "OVUX1VhGj9W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have introduced one more parameter called temperature, which is used to indicate how hard we should stick to the highest probability. If temperature is 1.0, we do fair multinomial sampling, and when temperature goes to infinity - all probabilities become equal, and we randomly select next character. In the example below we can observe that the text becomes meaningless when we increase the temperature too much, and it resembles \"cycled\" hard-generated text when it becomes closer to 0."
      ],
      "metadata": {
        "id": "jQ8hMU8fluwD"
      }
    }
  ]
}