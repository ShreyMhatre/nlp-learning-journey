{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6M0ITfKIvv01biRCjT33Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyMhatre/nlp-learning-journey/blob/main/NLP_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset (News AG)"
      ],
      "metadata": {
        "id": "fb89Ki6xMJAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-59cT49tLqVI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a classifier"
      ],
      "metadata": {
        "id": "UqoPkhlxMOWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "vocab_size = 30000\n",
        "batch_size = 128\n",
        "\n",
        "vectorizer = keras.layers.TextVectorization(max_tokens=vocab_size, output_mode='int')\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,\n",
        "    keras.layers.Embedding(vocab_size,100),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "dl9ozOwTMN8s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "EKDxRBgjPCsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "print(\"Training vectorizer\")\n",
        "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aaq-Ys7UPEVc",
        "outputId": "1520250b-d32b-4241-b7ea-47251f2c8e18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vectorizer\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - acc: 0.2514 - loss: 1.3863 - val_acc: 0.2500 - val_loss: 1.3863\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7afc4253d1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "4417kl_LTTVR",
        "outputId": "be862c9a-9e39-4868-db0b-6e7d7fb29651"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │     \u001b[38;5;34m3,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m404\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">404</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,000,810\u001b[0m (22.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,810</span> (22.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,000,404\u001b[0m (11.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,404</span> (11.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,000,406\u001b[0m (11.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,406</span> (11.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer('Hello, world!'))\n",
        "print(vectorizer('I am glad to meet you to to!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypNRWz37Tvnh",
        "outputId": "16078ab5-1167-43d4-f051-3646ffb37e5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
            "tf.Tensor([ 112 1271    1    3 1747  158    3    3], shape=(8,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer(['Hello, world!','I am glad to meet you!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2dIpJzUWIT",
        "outputId": "3a2e580f-b1cd-4232-f10a-c4c757ba12cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
              "array([[   1,   45,    0,    0,    0,    0],\n",
              "       [ 112, 1271,    1,    3, 1747,  158]])>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer(['Hello glad'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfElsBn5UetW",
        "outputId": "10396cb2-8598-42f4-f6ad-58caaeec898d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[1, 1]])>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic embeddings: Word2Vec"
      ],
      "metadata": {
        "id": "9b-jSic7U29H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy -q"
      ],
      "metadata": {
        "id": "KDrNSCmiWzIW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy>=2.0.0,<2.3.0 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IMEeN80W4UV",
        "outputId": "a727ba07-5906-430c-9bcb-e40140351c44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2.3.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8AM_e6KVrKq",
        "outputId": "022b0992-4e7a-4718-e675-ebc3f443c8af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd1U6_4qU38E",
        "outputId": "0097aa44-1def-4a15-dac4-0506f56feede"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('neural'):\n",
        "    print(f\"{w} -> {p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIkIkHfFXP72",
        "outputId": "e8cc7878-0f03-41af-dc9f-147ba7e42d6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neuronal -> 0.780479907989502\n",
            "neurons -> 0.7326499223709106\n",
            "neural_circuits -> 0.7252850532531738\n",
            "neuron -> 0.7174385190010071\n",
            "cortical -> 0.6941086649894714\n",
            "brain_circuitry -> 0.6923245787620544\n",
            "synaptic -> 0.6699119210243225\n",
            "neural_circuitry -> 0.6638563275337219\n",
            "neurochemical -> 0.6555314660072327\n",
            "neuronal_activity -> 0.6531826853752136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v['play'][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq0BdL3pXUfY",
        "outputId": "80110fe3-3465-41da-d265-660df8cbcd40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.01226807,  0.06225586,  0.10693359,  0.05810547,  0.23828125,\n",
              "        0.03686523,  0.05151367, -0.20703125,  0.01989746,  0.10058594,\n",
              "       -0.03759766, -0.1015625 , -0.15820312, -0.08105469, -0.0390625 ,\n",
              "       -0.05053711,  0.16015625,  0.2578125 ,  0.10058594, -0.25976562],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM2kx_MMXWmf",
        "outputId": "411f530c-1a62-4861-c8aa-93ecd1bce57f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('queen', 0.7118193507194519)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# get the vector corresponding to kind-man+woman\n",
        "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
        "# find the index of the closest embedding vector\n",
        "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
        "min_idx = np.argmin(d)\n",
        "# find the corresponding word\n",
        "w2v.index_to_key[min_idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "elJainquXaGs",
        "outputId": "f5d96ff8-0b8b-45e3-d063-88b0246ad3f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'queen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using pretrained embeddings in Keras"
      ],
      "metadata": {
        "id": "zFhFS7T9dMh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using tokenizer vocabulary"
      ],
      "metadata": {
        "id": "771nP6r2dg7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using the tokenizer vocabulary, some of the words from the vocabulary will have corresponding Word2Vec embeddings, and some will be missing. Given that our vocabulary size is vocab_size, and the Word2Vec embedding vector length is embed_size, the embedding layer will be repesented by a weight matrix of shape vocab_sizeembed_size. We will populate this matrix by going through the vocabulary:"
      ],
      "metadata": {
        "id": "c5UmtKyrhZox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "W = np.zeros((vocab_size,embed_size))\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found = 0,0\n",
        "for i,w in enumerate(vocab):\n",
        "    try:\n",
        "        W[i] = w2v.get_vector(w)\n",
        "        found+=1\n",
        "    except:\n",
        "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
        "        not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13BBty9hdiLK",
        "outputId": "6f99d3e9-9080-4f69-aa54-03449de47b16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding size: 300\n",
            "Populating matrix, this will take some time...Done, found 0 words, 2 words missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer, emb,\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "IcsRgQSDgB_-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
        "          validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjR2RAj7gIqt",
        "outputId": "6d407e4f-00f3-4589-dd81-76d363ae4326"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - acc: 0.2505 - loss: 1.3863 - val_acc: 0.2500 - val_loss: 1.3863\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7afc425f9610>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using embedding vocabulary"
      ],
      "metadata": {
        "id": "NMDO8Xbzgo0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One issue with the previous approach is that the vocabularies used in the TextVectorization and Embedding are different. To overcome this problem, we can use one of the following solutions:\n",
        "\n",
        "    Re-train the Word2Vec model on our vocabulary.\n",
        "    Load our dataset with the vocabulary from the pretrained Word2Vec model. Vocabularies used to load the dataset can be specified during loading.\n",
        "\n",
        "The latter approach seems easier, so let's implement it. First of all, we will create a TextVectorization layer with the specified vocabulary, taken from the Word2Vec embeddings:"
      ],
      "metadata": {
        "id": "VoOS9M3rhdQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(w2v.key_to_index.keys())\n",
        "vectorizer = keras.layers.TextVectorization(input_shape=(1,))\n",
        "vectorizer.set_vocabulary(vocab)"
      ],
      "metadata": {
        "id": "3eqy007aguGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = w2v.vector_size\n",
        "vocab_size = len(vocab) # Use the size of the vocabulary set in the vectorizer\n",
        "\n",
        "emb = keras.layers.Embedding(vocab_size, embed_size, weights=[w2v.vectors], trainable=False)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,\n",
        "    emb, # Use the created Embedding layer\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).padded_batch(batch_size, padded_shapes=([vocab_size], [])),\n",
        "          validation_data=ds_test.map(tupelize).padded_batch(batch_size, padded_shapes=([vocab_size], [])),\n",
        "          epochs=5)"
      ],
      "metadata": {
        "id": "wtheTr8qhguf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}